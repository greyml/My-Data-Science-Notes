------------------------- 
Tags: #ml 
Date Created:  2023-07-12, @ 16:14

---
>[!info] Keywords
>* 






### Maths behind it: 

$$pred = h_1(\theta)\alpha + h_2(\theta)\alpha + ... + h_n(\theta)\alpha$$
- `alpha` is **learning rate** [0, 1] and it's value will be same for all DT. 
- In the case of gradient boosting DT have full depth (mostly in between 8 - 32) and it is controlled with `max_depth=` in Scikit Learn
- `aplha ` helps to reduce over fitting.
- 

$$RES n = Actual - (M_{0} + \alpha M_{1}+\dots+ M_{n-1})$$



>[!summary] 
>1. 
>2. 

----
>[!cite]
> Connect with Me: https://www.linkedin.com/in/omkarpawar1430/
> My YouTube Channel: 
> [ChatGPT](https://chat.openai.com/)
> 
> 
